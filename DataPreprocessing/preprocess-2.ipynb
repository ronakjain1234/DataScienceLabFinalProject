{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ronak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ronak\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import kagglehub\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tabulate import tabulate\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import seaborn as sns\n",
    "from textblob import TextBlob\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import textstat\n",
    "\n",
    "# Ensure NLTK resources are downloaded\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\ronak\\.cache\\kagglehub\\datasets\\saurabhshahane\\fake-news-classification\\versions\\77\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>LAW ENFORCEMENT ON HIGH ALERT Following Threat...</td>\n",
       "      <td>No comment is expected from Barack Obama Membe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Did they post their votes for Hillary already?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>UNBELIEVABLE! OBAMA’S ATTORNEY GENERAL SAYS MO...</td>\n",
       "      <td>Now, most of the demonstrators gathered last ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Bobby Jindal, raised Hindu, uses story of Chri...</td>\n",
       "      <td>A dozen politically active pastors came here f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>SATAN 2: Russia unvelis an image of its terrif...</td>\n",
       "      <td>The RS-28 Sarmat missile, dubbed Satan 2, will...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "0           0  LAW ENFORCEMENT ON HIGH ALERT Following Threat...   \n",
       "1           1                                                NaN   \n",
       "2           2  UNBELIEVABLE! OBAMA’S ATTORNEY GENERAL SAYS MO...   \n",
       "3           3  Bobby Jindal, raised Hindu, uses story of Chri...   \n",
       "4           4  SATAN 2: Russia unvelis an image of its terrif...   \n",
       "\n",
       "                                                text  label  \n",
       "0  No comment is expected from Barack Obama Membe...      1  \n",
       "1     Did they post their votes for Hillary already?      1  \n",
       "2   Now, most of the demonstrators gathered last ...      1  \n",
       "3  A dozen politically active pastors came here f...      0  \n",
       "4  The RS-28 Sarmat missile, dubbed Satan 2, will...      1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"saurabhshahane/fake-news-classification\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "# load datasets\n",
    "df=pd.read_csv(path+'/WELFake_Dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LAW ENFORCEMENT ON HIGH ALERT Following Threat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UNBELIEVABLE! OBAMA’S ATTORNEY GENERAL SAYS MO...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bobby Jindal, raised Hindu, uses story of Chri...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SATAN 2: Russia unvelis an image of its terrif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             article  label\n",
       "0  LAW ENFORCEMENT ON HIGH ALERT Following Threat...      1\n",
       "1                                                NaN      1\n",
       "2  UNBELIEVABLE! OBAMA’S ATTORNEY GENERAL SAYS MO...      1\n",
       "3  Bobby Jindal, raised Hindu, uses story of Chri...      0\n",
       "4  SATAN 2: Russia unvelis an image of its terrif...      1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['article'] = df['title'] + df['text']\n",
    "df = df[['article', 'label']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63121, 2)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna(inplace=True)\n",
    "df.drop_duplicates(subset='article', inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment_polarity'] = df['article'].apply(lambda x: TextBlob(str(x)).sentiment.polarity)\n",
    "df['sentiment_subjectivity'] = df['article'].apply(lambda x: TextBlob(str(x)).sentiment.subjectivity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['word_count'] = df['article'].apply(lambda x: len(str(x).split()))\n",
    "df['char_count'] = df['article'].apply(lambda x: len(str(x)))\n",
    "df['avg_word_length'] = df['char_count'] / (df['word_count'] + 1)\n",
    "df['sentence_count'] = df['article'].apply(lambda x: len(str(x).split('.')))\n",
    "df['unique_word_count'] = df['article'].apply(lambda x: len(set(str(x).split())) / len(str(x).split()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['flesch_reading_ease'] = df['article'].apply(lambda x: textstat.flesch_reading_ease(str(x)))\n",
    "df['exclamation_count'] = df['article'].apply(lambda x: str(x).count('!'))\n",
    "df['question_count'] = df['article'].apply(lambda x: str(x).count('?'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of overlapping samples between train and test sets: 0\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['article'], df['label'], test_size=0.2, random_state=42, stratify=df['label'])\n",
    "overlap = set(X_train).intersection(set(X_test))\n",
    "print(f\"Number of overlapping samples between train and test sets: {len(overlap)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.94      0.94      6959\n",
      "           1       0.93      0.93      0.93      5666\n",
      "\n",
      "    accuracy                           0.94     12625\n",
      "   macro avg       0.94      0.94      0.94     12625\n",
      "weighted avg       0.94      0.94      0.94     12625\n",
      "\n",
      "Test Set Accuracy: 0.9365544554455446\n"
     ]
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\b([a-zA-Z])\\.(?=[a-zA-Z])', r'\\1specialdot', text)  # Preserve acronyms\n",
    "    text = re.sub(r'\\b([a-zA-Z])\\.(?=\\s|$)', r'\\1specialdot', text)\n",
    "    text = re.sub(r'http\\S+|www\\S+|[\\w\\.-]+@[\\w\\.-]+', '', text) # remove emails\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove Numbers\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
    "    text = text.replace(\"specialdot\", \".\")\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Tokenize, remove stopwords, and lemmatize\n",
    "    tokens = text.split()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.discard(\"not\")\n",
    "    stop_words.discard(\"no\")\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Vectorize text\n",
    "def vectorize(text, maxFeats, ngram):\n",
    "    vectorizer = TfidfVectorizer(max_features=maxFeats, ngram_range=ngram, stop_words='english')\n",
    "    X_tfidf = vectorizer.fit_transform(text)\n",
    "    return X_tfidf, vectorizer\n",
    "\n",
    "\n",
    "# Load data\n",
    "# Load data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[['article','sentiment_polarity','sentiment_subjectivity','word_count','char_count','avg_word_length','sentence_count','unique_word_count','flesch_reading_ease','exclamation_count','question_count']], df['label'], \n",
    "    test_size=0.2, random_state=42, stratify=df['label']\n",
    ")\n",
    "\n",
    "# Preprocess data\n",
    "X_train_processed = X_train['article'].apply(preprocess_text)\n",
    "X_test_processed = X_test['article'].apply(preprocess_text)\n",
    "\n",
    "# Vectorize data\n",
    "X_train_vectorized, vectorizer = vectorize(X_train_processed, 2000, (1,1))\n",
    "X_test_vectorized = vectorizer.transform(X_test_processed)\n",
    "\n",
    "# Preprocess data\n",
    "X_train_processed = X_train.apply(preprocess_text)\n",
    "X_test_processed = X_test.apply(preprocess_text)\n",
    "\n",
    "# Vectorize data\n",
    "X_train_vectorized, vectorizer = vectorize(X_train_processed, 2000, (1,1))\n",
    "X_test_vectorized = vectorizer.transform(X_test_processed)\n",
    "\n",
    "# Train model\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Predict and evaluate on test set\n",
    "y_pred = model.predict(X_test_vectorized)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f\"Test Set Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
    "\n",
    "# # Cross-validation\n",
    "# scores = cross_val_score(model, X_train_vectorized, y_train, cv=5, scoring='accuracy')\n",
    "# print(\"Cross-validation Accuracies:\", scores)\n",
    "# print(\"Average Accuracy:\", scores.mean())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
